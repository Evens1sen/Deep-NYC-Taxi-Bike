nohup: ignoring input
data.shape (17544, 69)
pred_METR-LA_GraphWaveNet_220109000225 training started Sun Jan  9 00:02:26 2022
TRAIN XS.shape YS,shape (14021, 2, 69, 12) (14021, 3, 69, 1)
Model Training Started ... Sun Jan  9 00:02:26 2022
TIMESTEP_IN, TIMESTEP_OUT 12 3
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
├─Conv2d: 1-1                            [-1, 32, 69, 13]          96
├─ModuleList: 1                          []                        --
|    └─Conv2d: 2-1                       [-1, 32, 69, 12]          2,080
├─ModuleList: 1                          []                        --
|    └─Conv1d: 2-2                       [-1, 32, 69, 12]          2,080
├─ModuleList: 1                          []                        --
|    └─Conv1d: 2-3                       [-1, 256, 69, 12]         8,448
├─ModuleList: 1                          []                        --
|    └─gcn: 2-4                          [-1, 32, 69, 12]          --
|    |    └─nconv: 3-1                   [-1, 32, 69, 12]          --
|    |    └─nconv: 3-2                   [-1, 32, 69, 12]          --
|    |    └─nconv: 3-3                   [-1, 32, 69, 12]          --
|    |    └─nconv: 3-4                   [-1, 32, 69, 12]          --
|    |    └─nconv: 3-5                   [-1, 32, 69, 12]          --
|    |    └─nconv: 3-6                   [-1, 32, 69, 12]          --
|    |    └─linear: 3-7                  [-1, 32, 69, 12]          7,200
├─ModuleList: 1                          []                        --
|    └─BatchNorm2d: 2-5                  [-1, 32, 69, 12]          64
├─ModuleList: 1                          []                        --
|    └─Conv2d: 2-6                       [-1, 32, 69, 10]          2,080
├─ModuleList: 1                          []                        --
|    └─Conv1d: 2-7                       [-1, 32, 69, 10]          2,080
├─ModuleList: 1                          []                        --
|    └─Conv1d: 2-8                       [-1, 256, 69, 10]         8,448
├─ModuleList: 1                          []                        --
|    └─gcn: 2-9                          [-1, 32, 69, 10]          --
|    |    └─nconv: 3-8                   [-1, 32, 69, 10]          --
|    |    └─nconv: 3-9                   [-1, 32, 69, 10]          --
|    |    └─nconv: 3-10                  [-1, 32, 69, 10]          --
|    |    └─nconv: 3-11                  [-1, 32, 69, 10]          --
|    |    └─nconv: 3-12                  [-1, 32, 69, 10]          --
|    |    └─nconv: 3-13                  [-1, 32, 69, 10]          --
|    |    └─linear: 3-14                 [-1, 32, 69, 10]          7,200
├─ModuleList: 1                          []                        --
|    └─BatchNorm2d: 2-10                 [-1, 32, 69, 10]          64
├─ModuleList: 1                          []                        --
|    └─Conv2d: 2-11                      [-1, 32, 69, 9]           2,080
├─ModuleList: 1                          []                        --
|    └─Conv1d: 2-12                      [-1, 32, 69, 9]           2,080
├─ModuleList: 1                          []                        --
|    └─Conv1d: 2-13                      [-1, 256, 69, 9]          8,448
├─ModuleList: 1                          []                        --
|    └─gcn: 2-14                         [-1, 32, 69, 9]           --
|    |    └─nconv: 3-15                  [-1, 32, 69, 9]           --
|    |    └─nconv: 3-16                  [-1, 32, 69, 9]           --
|    |    └─nconv: 3-17                  [-1, 32, 69, 9]           --
|    |    └─nconv: 3-18                  [-1, 32, 69, 9]           --
|    |    └─nconv: 3-19                  [-1, 32, 69, 9]           --
|    |    └─nconv: 3-20                  [-1, 32, 69, 9]           --
|    |    └─linear: 3-21                 [-1, 32, 69, 9]           7,200
├─ModuleList: 1                          []                        --
|    └─BatchNorm2d: 2-15                 [-1, 32, 69, 9]           64
├─ModuleList: 1                          []                        --
|    └─Conv2d: 2-16                      [-1, 32, 69, 7]           2,080
├─ModuleList: 1                          []                        --
|    └─Conv1d: 2-17                      [-1, 32, 69, 7]           2,080
├─ModuleList: 1                          []                        --
|    └─Conv1d: 2-18                      [-1, 256, 69, 7]          8,448
├─ModuleList: 1                          []                        --
|    └─gcn: 2-19                         [-1, 32, 69, 7]           --
|    |    └─nconv: 3-22                  [-1, 32, 69, 7]           --
|    |    └─nconv: 3-23                  [-1, 32, 69, 7]           --
|    |    └─nconv: 3-24                  [-1, 32, 69, 7]           --
|    |    └─nconv: 3-25                  [-1, 32, 69, 7]           --
|    |    └─nconv: 3-26                  [-1, 32, 69, 7]           --
|    |    └─nconv: 3-27                  [-1, 32, 69, 7]           --
|    |    └─linear: 3-28                 [-1, 32, 69, 7]           7,200
├─ModuleList: 1                          []                        --
|    └─BatchNorm2d: 2-20                 [-1, 32, 69, 7]           64
├─ModuleList: 1                          []                        --
|    └─Conv2d: 2-21                      [-1, 32, 69, 6]           2,080
├─ModuleList: 1                          []                        --
|    └─Conv1d: 2-22                      [-1, 32, 69, 6]           2,080
├─ModuleList: 1                          []                        --
|    └─Conv1d: 2-23                      [-1, 256, 69, 6]          8,448
├─ModuleList: 1                          []                        --
|    └─gcn: 2-24                         [-1, 32, 69, 6]           --
|    |    └─nconv: 3-29                  [-1, 32, 69, 6]           --
|    |    └─nconv: 3-30                  [-1, 32, 69, 6]           --
|    |    └─nconv: 3-31                  [-1, 32, 69, 6]           --
|    |    └─nconv: 3-32                  [-1, 32, 69, 6]           --
|    |    └─nconv: 3-33                  [-1, 32, 69, 6]           --
|    |    └─nconv: 3-34                  [-1, 32, 69, 6]           --
|    |    └─linear: 3-35                 [-1, 32, 69, 6]           7,200
├─ModuleList: 1                          []                        --
|    └─BatchNorm2d: 2-25                 [-1, 32, 69, 6]           64
├─ModuleList: 1                          []                        --
|    └─Conv2d: 2-26                      [-1, 32, 69, 4]           2,080
├─ModuleList: 1                          []                        --
|    └─Conv1d: 2-27                      [-1, 32, 69, 4]           2,080
├─ModuleList: 1                          []                        --
|    └─Conv1d: 2-28                      [-1, 256, 69, 4]          8,448
├─ModuleList: 1                          []                        --
|    └─gcn: 2-29                         [-1, 32, 69, 4]           --
|    |    └─nconv: 3-36                  [-1, 32, 69, 4]           --
|    |    └─nconv: 3-37                  [-1, 32, 69, 4]           --
|    |    └─nconv: 3-38                  [-1, 32, 69, 4]           --
|    |    └─nconv: 3-39                  [-1, 32, 69, 4]           --
|    |    └─nconv: 3-40                  [-1, 32, 69, 4]           --
|    |    └─nconv: 3-41                  [-1, 32, 69, 4]           --
|    |    └─linear: 3-42                 [-1, 32, 69, 4]           7,200
├─ModuleList: 1                          []                        --
|    └─BatchNorm2d: 2-30                 [-1, 32, 69, 4]           64
├─ModuleList: 1                          []                        --
|    └─Conv2d: 2-31                      [-1, 32, 69, 3]           2,080
├─ModuleList: 1                          []                        --
|    └─Conv1d: 2-32                      [-1, 32, 69, 3]           2,080
├─ModuleList: 1                          []                        --
|    └─Conv1d: 2-33                      [-1, 256, 69, 3]          8,448
├─ModuleList: 1                          []                        --
|    └─gcn: 2-34                         [-1, 32, 69, 3]           --
|    |    └─nconv: 3-43                  [-1, 32, 69, 3]           --
|    |    └─nconv: 3-44                  [-1, 32, 69, 3]           --
|    |    └─nconv: 3-45                  [-1, 32, 69, 3]           --
|    |    └─nconv: 3-46                  [-1, 32, 69, 3]           --
|    |    └─nconv: 3-47                  [-1, 32, 69, 3]           --
|    |    └─nconv: 3-48                  [-1, 32, 69, 3]           --
|    |    └─linear: 3-49                 [-1, 32, 69, 3]           7,200
├─ModuleList: 1                          []                        --
|    └─BatchNorm2d: 2-35                 [-1, 32, 69, 3]           64
├─ModuleList: 1                          []                        --
|    └─Conv2d: 2-36                      [-1, 32, 69, 1]           2,080
├─ModuleList: 1                          []                        --
|    └─Conv1d: 2-37                      [-1, 32, 69, 1]           2,080
├─ModuleList: 1                          []                        --
|    └─Conv1d: 2-38                      [-1, 256, 69, 1]          8,448
├─ModuleList: 1                          []                        --
|    └─gcn: 2-39                         [-1, 32, 69, 1]           --
|    |    └─nconv: 3-50                  [-1, 32, 69, 1]           --
|    |    └─nconv: 3-51                  [-1, 32, 69, 1]           --
|    |    └─nconv: 3-52                  [-1, 32, 69, 1]           --
|    |    └─nconv: 3-53                  [-1, 32, 69, 1]           --
|    |    └─nconv: 3-54                  [-1, 32, 69, 1]           --
|    |    └─nconv: 3-55                  [-1, 32, 69, 1]           --
|    |    └─linear: 3-56                 [-1, 32, 69, 1]           7,200
├─ModuleList: 1                          []                        --
|    └─BatchNorm2d: 2-40                 [-1, 32, 69, 1]           64
├─Conv2d: 1-2                            [-1, 512, 69, 1]          131,584
├─Conv2d: 1-3                            [-1, 3, 69, 1]            1,539
==========================================================================================
Total params: 292,195
Trainable params: 292,195
Non-trainable params: 0
Total mult-adds (M): 79.13
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 11.00
Params size (MB): 1.11
Estimated Total Size (MB): 12.12
==========================================================================================
XS_torch.shape:   torch.Size([14021, 2, 69, 12])
YS_torch.shape:   torch.Size([14021, 3, 69, 1])
LOSS is : MAE
epoch 0 time used: 69  seconds  train loss: 0.4185912925683212 validation loss: 0.35046818702817983
epoch 1 time used: 69  seconds  train loss: 0.36102417516599544 validation loss: 0.3220758340186006
epoch 2 time used: 70  seconds  train loss: 0.3350194714475152 validation loss: 0.30839209215204444
epoch 3 time used: 66  seconds  train loss: 0.3171295316183571 validation loss: 0.299733144245485
epoch 4 time used: 69  seconds  train loss: 0.30517504975700127 validation loss: 0.29589991738166527
epoch 5 time used: 70  seconds  train loss: 0.2981332001019115 validation loss: 0.2854227792754285
epoch 6 time used: 70  seconds  train loss: 0.28986021056212546 validation loss: 0.2800969365353048
epoch 7 time used: 69  seconds  train loss: 0.28363568170823916 validation loss: 0.2750592387817413
epoch 8 time used: 70  seconds  train loss: 0.2797600322606162 validation loss: 0.27287755652420603
epoch 9 time used: 78  seconds  train loss: 0.2760276368436263 validation loss: 0.2690425006342288
epoch 10 time used: 73  seconds  train loss: 0.2713229377533272 validation loss: 0.2636593089434193
epoch 11 time used: 70  seconds  train loss: 0.2696492799551976 validation loss: 0.26431122979843064
epoch 12 time used: 69  seconds  train loss: 0.2673431927387435 validation loss: 0.2613274585976304
epoch 13 time used: 70  seconds  train loss: 0.2640670571569748 validation loss: 0.255728874455434
epoch 14 time used: 69  seconds  train loss: 0.26168445496050746 validation loss: 0.25574342348850326
epoch 15 time used: 69  seconds  train loss: 0.2599704730265841 validation loss: 0.25269234647971184
epoch 16 time used: 70  seconds  train loss: 0.2579180108809028 validation loss: 0.2579582844267419
epoch 17 time used: 69  seconds  train loss: 0.2566386669300883 validation loss: 0.25193139604684356
epoch 18 time used: 69  seconds  train loss: 0.2561927931361996 validation loss: 0.25158565774075453
epoch 19 time used: 69  seconds  train loss: 0.2534419577052549 validation loss: 0.24824367447437998
epoch 20 time used: 71  seconds  train loss: 0.2517288937061821 validation loss: 0.24594481973667112
epoch 21 time used: 69  seconds  train loss: 0.25193737949635486 validation loss: 0.2472395923432798
epoch 22 time used: 69  seconds  train loss: 0.24893420002322109 validation loss: 0.2481676601744759
epoch 23 time used: 69  seconds  train loss: 0.2479194325152305 validation loss: 0.24219268949726822
epoch 24 time used: 70  seconds  train loss: 0.24680000093692983 validation loss: 0.2449312283016244
epoch 25 time used: 69  seconds  train loss: 0.24712516329052941 validation loss: 0.24291137284166256
epoch 26 time used: 77  seconds  train loss: 0.24454194817474623 validation loss: 0.23978072097147796
epoch 27 time used: 73  seconds  train loss: 0.2439252171224517 validation loss: 0.24421469217800645
epoch 28 time used: 69  seconds  train loss: 0.24368719324678484 validation loss: 0.2426081040605299
epoch 29 time used: 70  seconds  train loss: 0.24256017611747382 validation loss: 0.23785020501118556
epoch 30 time used: 69  seconds  train loss: 0.24192192994075137 validation loss: 0.24084990228915718
epoch 31 time used: 69  seconds  train loss: 0.2411922258636172 validation loss: 0.2366852832738155
epoch 32 time used: 70  seconds  train loss: 0.24087531487134056 validation loss: 0.23688822353967448
epoch 33 time used: 70  seconds  train loss: 0.2400915673434404 validation loss: 0.24307739930158334
epoch 34 time used: 70  seconds  train loss: 0.2403996927578052 validation loss: 0.23894636984967668
epoch 35 time used: 70  seconds  train loss: 0.23894545181317636 validation loss: 0.23573716039223733
epoch 36 time used: 70  seconds  train loss: 0.23812093721525965 validation loss: 0.2406341189394253
epoch 37 time used: 70  seconds  train loss: 0.2374466620324915 validation loss: 0.23360753372201087
epoch 38 time used: 72  seconds  train loss: 0.23701861153839854 validation loss: 0.23823672578937316
epoch 39 time used: 72  seconds  train loss: 0.23618060508320843 validation loss: 0.23535421887636865
epoch 40 time used: 71  seconds  train loss: 0.235954326077358 validation loss: 0.2358911175831209
epoch 41 time used: 70  seconds  train loss: 0.23447278241089745 validation loss: 0.23244371483207632
epoch 42 time used: 70  seconds  train loss: 0.23432782575133418 validation loss: 0.23271832722666871
epoch 43 time used: 70  seconds  train loss: 0.23443617452364773 validation loss: 0.23126266996179115
epoch 44 time used: 70  seconds  train loss: 0.23298349699602477 validation loss: 0.23658329188075666
epoch 45 time used: 70  seconds  train loss: 0.2336376347825937 validation loss: 0.2304277387402497
epoch 46 time used: 74  seconds  train loss: 0.232577479586253 validation loss: 0.23260906870815323
epoch 47 time used: 76  seconds  train loss: 0.2321709579322639 validation loss: 0.2339819776913267
epoch 48 time used: 70  seconds  train loss: 0.2314932053859513 validation loss: 0.231206640575657
epoch 49 time used: 71  seconds  train loss: 0.23211026028950127 validation loss: 0.22991920173746208
epoch 50 time used: 70  seconds  train loss: 0.23081678671948794 validation loss: 0.23134247191076066
epoch 51 time used: 70  seconds  train loss: 0.23061962041397766 validation loss: 0.2310884215120445
epoch 52 time used: 71  seconds  train loss: 0.23000287530105046 validation loss: 0.2291288039664575
epoch 53 time used: 70  seconds  train loss: 0.2298825253365986 validation loss: 0.23202577310667946
epoch 54 time used: 70  seconds  train loss: 0.22982809398922113 validation loss: 0.22956145894371027
epoch 55 time used: 69  seconds  train loss: 0.22898079924234396 validation loss: 0.22740581161589604
epoch 56 time used: 70  seconds  train loss: 0.2289099250150341 validation loss: 0.22658775721321225
epoch 57 time used: 76  seconds  train loss: 0.22778757504892924 validation loss: 0.2284315164879398
epoch 58 time used: 69  seconds  train loss: 0.22757106469922864 validation loss: 0.22835102941707006
epoch 59 time used: 69  seconds  train loss: 0.22760847069986254 validation loss: 0.22508839661913196
epoch 60 time used: 69  seconds  train loss: 0.2273119677539598 validation loss: 0.22570180311518537
epoch 61 time used: 69  seconds  train loss: 0.2262677964166822 validation loss: 0.22451357597870888
epoch 62 time used: 69  seconds  train loss: 0.22626457628400776 validation loss: 0.22429432954981338
epoch 63 time used: 70  seconds  train loss: 0.22605820239697988 validation loss: 0.22770451562648084
epoch 64 time used: 69  seconds  train loss: 0.22568055329996792 validation loss: 0.22413342656473126
epoch 65 time used: 69  seconds  train loss: 0.22582209646157655 validation loss: 0.22401336701645283
epoch 66 time used: 69  seconds  train loss: 0.2251925423940541 validation loss: 0.2284449063245732
epoch 67 time used: 69  seconds  train loss: 0.22443445830963937 validation loss: 0.2241529861621291
epoch 68 time used: 69  seconds  train loss: 0.22388544592205928 validation loss: 0.2243506462996984
epoch 69 time used: 70  seconds  train loss: 0.2247632904446471 validation loss: 0.22355744003807554
epoch 70 time used: 70  seconds  train loss: 0.2235383850318584 validation loss: 0.22735388872797532
epoch 71 time used: 70  seconds  train loss: 0.22305706659356817 validation loss: 0.22667363103260263
epoch 72 time used: 70  seconds  train loss: 0.2231554403526992 validation loss: 0.22460061948400054
epoch 73 time used: 70  seconds  train loss: 0.22272357017161368 validation loss: 0.22246667241206933
epoch 74 time used: 70  seconds  train loss: 0.22282441586188007 validation loss: 0.22454114814214005
epoch 75 time used: 70  seconds  train loss: 0.2219478153894808 validation loss: 0.22289972108429115
epoch 76 time used: 69  seconds  train loss: 0.22160667808739185 validation loss: 0.2218647326868191
epoch 77 time used: 70  seconds  train loss: 0.22129656057677285 validation loss: 0.22489451093804272
epoch 78 time used: 69  seconds  train loss: 0.22153068082434446 validation loss: 0.22163799660109276
epoch 79 time used: 69  seconds  train loss: 0.22081656359502924 validation loss: 0.22180017548428899
epoch 80 time used: 69  seconds  train loss: 0.2207247125021907 validation loss: 0.22148382171554967
epoch 81 time used: 69  seconds  train loss: 0.22116740375397684 validation loss: 0.22040391590550362
epoch 82 time used: 69  seconds  train loss: 0.22013090112172784 validation loss: 0.2201506451868834
epoch 83 time used: 70  seconds  train loss: 0.22035304013606893 validation loss: 0.2197553967654331
epoch 84 time used: 70  seconds  train loss: 0.2197280425200604 validation loss: 0.21955589184200702
epoch 85 time used: 69  seconds  train loss: 0.21990805261388524 validation loss: 0.2191125971806505
epoch 86 time used: 70  seconds  train loss: 0.21846939116609496 validation loss: 0.2183123191492393
epoch 87 time used: 76  seconds  train loss: 0.2188797998513855 validation loss: 0.21864454475424322
epoch 88 time used: 71  seconds  train loss: 0.2187362470007269 validation loss: 0.218987445294415
epoch 89 time used: 70  seconds  train loss: 0.21818021366407322 validation loss: 0.21879412476940285
epoch 90 time used: 71  seconds  train loss: 0.21879095866192735 validation loss: 0.22175455340744765
Early stopping at epoch: 91
YS.shape, YS_pred.shape before, (14021, 3, 69, 1) (14021, 3, 69, 1)
YS.shape, YS_pred.shape after, (14021, 3, 69) (14021, 3, 69)
YS_pred.shape before (14021, 3, 69)
YS_pred.shape after (14021, 3, 69)
YS.shape, YS_pred.shape, (14021, 3, 69) (14021, 3, 69)
****************************************
GraphWaveNet, train, Torch MSE, 2.2194797917e-01, 0.2219479792
GraphWaveNet, train, MSE, RMSE, MAE, MAPE, 675.1511229322, 25.9836703130, 14.6348838032, 21.2003302828
Model Training Ended ... Sun Jan  9 01:52:48 2022
pred_METR-LA_GraphWaveNet_220109000225 testing started Sun Jan  9 01:52:48 2022
TEST XS.shape, YS.shape (3507, 2, 69, 12) (3507, 3, 69, 1)
Model Testing Started ... Sun Jan  9 01:52:48 2022
TIMESTEP_IN, TIMESTEP_OUT 12 3
YS.shape, YS_pred.shape before, (3507, 3, 69, 1) (3507, 3, 69, 1)
YS.shape, YS_pred.shape after, (3507, 3, 69) (3507, 3, 69)
YS.shape, YS_pred.shape, (3507, 3, 69) (3507, 3, 69)
****************************************
GraphWaveNet, test, Torch MSE, 2.1070779745e-01, 0.2107077975
all pred steps, GraphWaveNet, test, MSE, RMSE, MAE, MAPE, 669.6633484979, 25.8778544029, 14.2124774246, 21.4560122632
1 step, GraphWaveNet, test, MSE, RMSE, MAE, MAPE, 540.2856971616, 23.2440464885, 13.0117308992, 20.1498580309
2 step, GraphWaveNet, test, MSE, RMSE, MAE, MAPE, 675.8193177630, 25.9965251094, 14.3193347653, 21.5134965152
3 step, GraphWaveNet, test, MSE, RMSE, MAE, MAPE, 792.8842214942, 28.1581998980, 15.3063569567, 22.7046710332
Model Testing Ended ... Sun Jan  9 01:53:17 2022
